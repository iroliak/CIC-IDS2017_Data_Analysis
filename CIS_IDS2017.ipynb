{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Πρακτική Εφαρμογή: Ανίχνευση Απειλών και Ανωμαλιών στο Σκοτεινό Διαδίκτυο\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1FFia7MZlhWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and Concatenating the CSV Files"
      ],
      "metadata": {
        "id": "oD035HmAmQq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Define paths\n",
        "zip_file_path = 'path_to_your_zip_file/MachineLearningCSV.zip'  # Change this to your local path\n",
        "extract_dir = 'path_to_extracted_files/cicids2017/'  # Change this to your desired extraction path\n",
        "\n",
        "# Extract the ZIP file\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# List CSV files in the directory\n",
        "csv_files_path = os.path.join(extract_dir, 'MachineLearningCVE')\n",
        "csv_files = os.listdir(csv_files_path)\n",
        "\n",
        "# Load and concatenate the CSV files into a single DataFrame\n",
        "df_list = [pd.read_csv(os.path.join(csv_files_path, file)) for file in csv_files]\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Display the shape and first few rows\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JpokQG0WmT77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-processing"
      ],
      "metadata": {
        "id": "98skkP7fmb71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and retain useful columns\n",
        "# Example: Drop columns that may not be useful, like IP addresses or those with too many missing values\n",
        "df = df.dropna(axis=1, how='any')  # Example: Dropping columns with any NaN values\n",
        "df = df.select_dtypes(include=[int, float])  # Example: Keeping only numeric columns\n",
        "\n",
        "# Display the updated DataFrame structure\n",
        "print(f\"Preprocessed dataset shape: {df.shape}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BMbzkED-mc3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Dataset"
      ],
      "metadata": {
        "id": "fjVitElrmgHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume the target variable is the last column\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "vlIx66tgmimC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling Numerical Features"
      ],
      "metadata": {
        "id": "2ja44ebmmlIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and test sets\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "jvQ5hXWsmpbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical Features (if any)"
      ],
      "metadata": {
        "id": "SgsuyGO6mr-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Example: Apply OneHotEncoder if there are categorical columns\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Fit and transform on training set, then transform on the test set\n",
        "# Note: Adapt this code to your specific dataset\n",
        "# X_train_encoded = encoder.fit_transform(X_train_categorical)\n",
        "# X_test_encoded = encoder.transform(X_test_categorical)\n"
      ],
      "metadata": {
        "id": "HQob5OMtmvpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction"
      ],
      "metadata": {
        "id": "tPSuIDQ9myI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Example: Applying PCA for dimensionality reduction (optional)\n",
        "pca = PCA(n_components=10)  # Example: Reduce to 10 components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "oS49YdrJm0x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Classifiers"
      ],
      "metadata": {
        "id": "Vgm-xoqQm3IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize classifiers\n",
        "models = {\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'K-Neighbors': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_pca, y_train)  # Use X_train_scaled if not using PCA\n",
        "    y_pred = model.predict(X_test_pca)  # Use X_test_scaled if not using PCA\n",
        "    print(f\"Results for {name}:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "57xrU_QWm5mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result Analysis"
      ],
      "metadata": {
        "id": "qfUUHT-hm8S6"
      }
    }
  ]
}